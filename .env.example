# =======================================
# Stock Market ELT - Example Environment
# =======================================

# -------------------------
# --- Docker Compose ---
# -------------------------
COMPOSE_PROJECT_NAME=stocks_elt
COMPOSE_CONVERT_WINDOWS_PATHS=1

# -------------------------
# --- AWS Configuration ---
# -------------------------
AWS_DEFAULT_REGION=us-east-2
AWS_PROFILE=default
BUCKET_NAME=stock-market-elt

# Allow SDKs (boto3, Airflow, dbt, etc.) to auto-detect profiles from ~/.aws/config
AWS_SDK_LOAD_CONFIG=1

# -------------------------
# --- Manifest Management ---
# -------------------------
# Pointer to latest manifest file in S3 (used by backfill and daily ingest DAGs)
STOCKS_MANIFEST_KEY=raw/manifests/manifest_latest.txt

# Prefix for daily manifest files
STOCKS_DAILY_MANIFEST_PREFIX=raw/manifests/stocks/daily

# -------------------------
# --- Airflow ↔ AWS Secrets Manager ---
# -------------------------
# Centralized secret management for connections, variables, and configs
AIRFLOW__SECRETS__BACKEND=airflow.providers.amazon.aws.secrets.secrets_manager.SecretsManagerBackend
AIRFLOW__SECRETS__BACKEND_KWARGS={"connections_prefix":"airflow/connections","variables_prefix":"airflow/variables","config_prefix":"airflow/config","region_name":"us-east-2"}

# -------------------------
# --- Snowflake Configuration ---
# -------------------------
# ❗ All credentials (account, user, password, role, etc.) are securely managed in:
#   AWS Secrets Manager → airflow/connections/snowflake_default
#
# Example extras JSON (for reference only):
# {
#   "account": "your_account",
#   "warehouse": "STOCKS_ELT_WH",
#   "database": "STOCKS_ELT_DB",
#   "schema": "PUBLIC",
#   "role": "STOCKS_ELT_ROLE",
#   "stage": "s3_stage"
# }

# Optional: used only by the dashboard container to override warehouse size
SNOWFLAKE_OVERRIDE_WAREHOUSE=STOCKS_DASHBOARD_WH

# -------------------------
# --- Airflow & dbt Core ---
# -------------------------
# dbt project and executable paths (must align with Dockerfile)
DBT_PROJECT_DIR=/usr/local/airflow/dbt
DBT_EXECUTABLE_PATH=/usr/local/airflow/dbt_venv/bin/dbt

# -------------------------
# --- dbt Build Controls ---
# -------------------------
# Target profile (fallbacks: Airflow Variable `dbt_target` → default "ci")
DBT_TARGET=ci

# Selection filters (e.g., "state:modified+", "tag:marts")
DBT_SELECT=
DBT_EXCLUDE=

# Toggle full-refresh for models/snapshots
DBT_FULL_REFRESH=false

# Optional runtime variables (JSON string)
# Example: {"run_mode":"prod","batch_date":"2025-10-15"}
DBT_VARS_JSON=

# Use latest S3 manifest for slim CI/partial parsing
DBT_USE_STATE=true

# Parallel threads for dbt execution (default: 4)
DBT_THREADS=4

# -------------------------
# --- Airflow Core Settings ---
# -------------------------
AIRFLOW__CORE__PARALLELISM=32
AIRFLOW__CORE__MAX_ACTIVE_RUNS_PER_DAG=24
AIRFLOW__CORE__MAX_ACTIVE_TASKS_PER_DAG=20
AIRFLOW__CORE__MAX_MAP_LENGTH=8000

# -------------------------
# --- Airflow Scheduler Settings ---
# -------------------------
AIRFLOW__SCHEDULER__DAG_DIR_LIST_INTERVAL=30
AIRFLOW__SCHEDULER__MIN_FILE_PROCESS_INTERVAL=30

# -------------------------
# --- HTTP / Network Tunables ---
# -------------------------
# Custom user agent for vendor tracking (Polygon.io)
HTTP_USER_AGENT=stocks-elt/polygon-options-dag (daily)

# Ingestion + request settings (used by both stocks and options DAGs)
HTTP_REQUEST_TIMEOUT_SECS=60
POLYGON_REQUEST_DELAY_SECONDS=0.25
