# =======================================
# Project: Stock Market ELT
# Example Environment Variables
# =======================================

# -------------------------
# --- Docker Compose ---
# -------------------------
COMPOSE_PROJECT_NAME=stocks_elt
COMPOSE_CONVERT_WINDOWS_PATHS=1

# -------------------------
# --- AWS Configuration ---
# -------------------------
AWS_DEFAULT_REGION=us-east-2
AWS_PROFILE=default
BUCKET_NAME=stock-market-elt

# --- AWS SDK behavior (so region/profile in ~/.aws/config are read) ---
AWS_SDK_LOAD_CONFIG=1

# --- Manifest behavior (loader understands pointer or flat manifest) ---
STOCKS_MANIFEST_KEY=raw/manifests/manifest_latest.txt
STOCKS_DAILY_MANIFEST_PREFIX=raw/manifests/stocks/daily

# Airflow ↔ AWS Secrets Manager Integration
# (For connections, variables, and configs)
AIRFLOW__SECRETS__BACKEND=airflow.providers.amazon.aws.secrets.secrets_manager.SecretsManagerBackend
AIRFLOW__SECRETS__BACKEND_KWARGS={"connections_prefix":"airflow/connections","variables_prefix":"airflow/variables","config_prefix":"airflow/config","region_name":"us-east-2"}

# ------------------------------------------------
# --- Snowflake Configuration (Managed via AWS) ---
# ------------------------------------------------
# ❗ No longer stored in .env
# All credentials and settings (account, user, password, warehouse, role, etc.)
# are securely stored in AWS Secrets Manager under:
#   airflow/connections/snowflake_default
#
# Example extras JSON (for reference only):
# {
#   "account": "your_account",
#   "warehouse": "STOCKS_ELT_WH",
#   "database": "STOCKS_ELT_DB",
#   "schema": "PUBLIC",
#   "role": "STOCKS_ELT_ROLE",
#   "stage": "s3_stage"
# }

# Optional: used only by the dashboard container to override warehouse size
SNOWFLAKE_OVERRIDE_WAREHOUSE=STOCKS_DASHBOARD_WH

# --------------------------
# --- Airflow & dbt Core ---
# --------------------------
# --- dbt runtime knobs (used by your dbt_build DAG) ---
DBT_PROJECT_DIR=/usr/local/airflow/dbt
DBT_EXECUTABLE_PATH=/usr/local/airflow/dbt_venv/bin/dbt
DBT_TARGET_PATH=/usr/local/airflow/dbt_target
DBT_THREADS=4

# --- Optional: Cosmos/dbt log format (handy if you parse logs) ---
DBT_LOG_FORMAT=json

# Airflow Core Settings
AIRFLOW__CORE__PARALLELISM=32
AIRFLOW__CORE__MAX_ACTIVE_RUNS_PER_DAG=24
AIRFLOW__CORE__MAX_ACTIVE_TASKS_PER_DAG=20
AIRFLOW__CORE__MAX_MAP_LENGTH=8000

# Airflow Scheduler Settings
AIRFLOW__SCHEDULER__DAG_DIR_LIST_INTERVAL=30
AIRFLOW__SCHEDULER__MIN_FILE_PROCESS_INTERVAL=30

# ---------------------------------
# --- Optional HTTP Tunables ---
# ---------------------------------
# Custom User-Agent for vendor support visibility
HTTP_USER_AGENT=stocks-elt/polygon-options-dag (daily)

# --- Ingestion/network tunables (stocks + options DAGs reuse these) ---
HTTP_REQUEST_TIMEOUT_SECS=60
POLYGON_REQUEST_DELAY_SECONDS=0.25
API_POOL=api_pool   # ensure this pool exists in Airflow
