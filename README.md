# Airflowâ€‘AWSâ€‘Snowflake Stock Market ELT

## Overview

This repository hosts the ELT (Extractâ€‘Loadâ€‘Transform) pipeline for stockâ€‘market data using Apacheâ€¯Airflow, dbt and Snowflake, built to run on AWS infrastructure.  
The project is structured in **phases**:

- Phaseâ€¯1 (v0.1.0): Local development and full endâ€‘toâ€‘end pipeline on Docker/Astro.  
- Phaseâ€¯2 (v1.0.0): Fully cloud operational â€“ infrastructure deployed and orchestrated in AWS (and Snowflake in production mode) with no local dependencies.  
- Phaseâ€¯3 (v2.0.0): Analytics dashboard layer (e.g., Plotlyâ€¯Dash) on top of the Snowflakeâ€‘dataâ€‘mart.

## ğŸ§© Project Versioning

| Version     | Status            | Goal / Acceptance Criteria                                                                 | Key Components                                                                                           |
|-------------|-------------------|--------------------------------------------------------------------------------------------|----------------------------------------------------------------------------------------------------------|
| **v0.1.0 â€” MVP (Local)** | âœ… In progress     | Pipeline runs full endâ€‘toâ€‘end **locally** (Docker Desktop / Astro) with data flowing: Polygonâ€¯â†’â€¯S3â€¯â†’â€¯Snowflakeâ€¯â†’â€¯dbt modelsâ€¯â†’â€¯marts. | Airflow (local), AWS S3 (dev), Snowflake (dev), dbtâ€¯Core, Secrets Manager, IAM roles/policies            |
| **v1.0.0 â€” Fully Cloud Operational** | â³ Next milestone | Pipeline runs **fully in the cloud** with no local dependencies. Infrastructure is deployed on AWS/Snowflake in productionâ€‘mode, with CI/CD, logging/monitoring, cost/ops readiness. | Managed Airflow (MWAA or Astro Cloud), AWS S3 (lifecycle policies), Snowflake (prod), dbt Cloud or containerized dbt, observability/logging, IAM, network/security |
| **v2.0.0 â€” Analytics Dashboard (Plotlyâ€¯Dash)** | ğŸ”œ Future        | Adds analytics/dashboard layer. A Plotlyâ€¯Dash (or Streamlit) app connects to the Snowflake marts and is hosted on AWS (Appâ€¯Runner or ECS) for endâ€‘user consumption. | Dash app, Snowflake connector, authentication/authorization, deployment pipeline, optional API gateway     |

## Architecture

*(You may update this section with architecture diagrams, AWS service lists, networking/VPC setup, etc.)*

1. **Data Extraction** â€” Pull stockâ€‘market data (e.g., from APIs) and store raw files in S3.  
2. **Data Ingestion** â€” Use Airflow DAGs to load raw files into Snowflake Staging.  
3. **Transformations** â€” Use dbt models to build marts and analytics tables in Snowflake.  
4. **Deployment & Ops (v1.0.0 target)** â€” Pipeline scheduled and managed in cloud via Managedâ€¯Airflow, secrets in AWS Secrets Manager, logging/monitoring via CloudWatch/thirdâ€‘party, CI/CD deployments via GitHub Actions or similar.  
5. **Dashboard Layer (v2.0.0)** â€” Visualization layer for stakeholders, powered by Plotlyâ€¯Dash, connecting to Snowflake marts.

## Getting Started (v0.1.0 Local)

*These steps assume you are running locally with Astro CLI / Docker.*

1. Clone the repository:

   ```bash
   git clone https://github.com/davehasacat/airflow-aws-snowflake-stock-market-elt.git
   cd airflow-aws-snowflake-stock-market-elt
   ```

2. Copy `.env_example` to `.env` and fill in your Snowflake, AWS and other credentials.

3. Start the local Airflow environment:

   ```bash
   astro dev start
   ```

4. Verify the DAGs, sources and target marts operate as expected.  
5. Once local pipeline is validated, move to v1.0.0 (cloud) architecture.

## Deployment for v1.0.0 (Fully Cloud)

*(Brief overview â€” detailed steps to be added)*

- Provision AWS resources (S3 buckets, IAM roles/policies, VPC, networking)  
- Configure Managed Airflow (e.g., MWAA or Astro Cloud)  
- Deploy Snowflake in production mode (warehouse sizing, security/roles, database/schema)  
- Configure dbt (Cloud or containerized) to run models in Snowflake  
- Setup CI/CD pipeline for DAG + DBT code changes  
- Setup monitoring, logging, alerting, cost controls, tagging, documentation  

## Roadmap & Next Steps

- Finalize infrastructure for v1.0.0 (target: cloudâ€‘first production).  
- Once v1.0.0 is green, move to v2.0.0: build and deploy dashboard layer.  
- Continuous improvements: dataâ€‘quality checks, lineage, observability, cost optimisation, governance.

## Contributing

Contributions, feedback, issues and feature requests are welcome. Please open a GitHub Issue for any bug or feature and submit a PR for changes.

## License

*(Insert your license information here)*
